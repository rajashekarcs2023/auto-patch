 To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['response_format'] in your request.
   Using fallback reward calculation...
⚠️ RULER evaluation failed: litellm.UnsupportedParamsError: openai does not support parameters: ['response_format'], for model=gpt-4. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['response_format'] in your request.
   Using fallback reward calculation...
⚠️ RULER evaluation failed: litellm.UnsupportedParamsError: openai does not support parameters: ['response_format'], for model=gpt-4. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['response_format'] in your request.
   Using fallback reward calculation...
Training model on reasoning improvements..